{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import datetime as dt\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import re\n",
    "import collections\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "source": [
    "# Text preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('text_processing', 'raw_data', 'expedia_eng_reviews.csv'))\n",
    "corpus = list(df.loc[:, 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessText:\n",
    "    \n",
    "    def __init__(self, raw_text):\n",
    "        self.text = raw_text\n",
    "        \n",
    "    # expand abbreviations\n",
    "    def decontract_words(self):\n",
    "        # punctuation mistake \n",
    "        phrase = re.sub(r\"’\", \"'\", self.text)\n",
    "        phrase = re.sub(r'\\\\', \"'\", self.text)\n",
    "\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "    \n",
    "    def tokenize_words(self, decontract=True):\n",
    "        if decontract:\n",
    "            text = self.decontract_words()\n",
    "        else:\n",
    "            text = self.text\n",
    "            \n",
    "        tokenized_text = word_tokenize(text)\n",
    "        return tokenized_text\n",
    "    \n",
    "    def preprocess_tokens(self, decontract=True):\n",
    "        words = self.tokenize_words(decontract)\n",
    "            \n",
    "        # create list of punctuation characters\n",
    "        punctuation = string.punctuation\n",
    "        customized_punctuation = ['“','”', '...', '', '’']\n",
    "        for punct in customized_punctuation:\n",
    "            punctuation += punct\n",
    "            \n",
    "        # lower words and remove punctuation\n",
    "        words = [word.lower().replace('\\n', '') for word in words if word not in punctuation]\n",
    "        words = [word for word in words if len(re.findall(r'\\d+', word)) == 0]\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def remove_stop_words_tokens(self, keep_stop_words=['most', 'very', 'not'], decontract=True):\n",
    "        words = self.preprocess_tokens(decontract)\n",
    "        \n",
    "        # define stop words\n",
    "        nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "        for stop_word in keep_stop_words:\n",
    "            nltk_stop_words.remove(stop_word)\n",
    "            \n",
    "        # return text without stop words\n",
    "        text_without_stop_words = [t for t in words if t not in nltk_stop_words]\n",
    "        \n",
    "        return text_without_stop_words\n",
    "    \n",
    "    def stemm_tokenized_words(self, keep_stop_words=['most', 'very', 'not'], decontract=True):\n",
    "        words = self.remove_stop_words_tokens(keep_stop_words, decontract)\n",
    "        \n",
    "        # initialize stemmers\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_words = []\n",
    "        for word in words:\n",
    "            stemmed_words.append(stemmer.stem(word))\n",
    "        \n",
    "        return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_corpus = [PreprocessText(raw_text) for raw_text in corpus]\n",
    "preprocessed_corpus = [text_item.stemm_tokenized_words() for text_item in preprocessed_corpus]"
   ]
  },
  {
   "source": [
    "# TD-IDF\n",
    "\n",
    "#### What is TD-IDF?\n",
    "\n",
    "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "It has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP).\n",
    "\n",
    "TF-IDF (term frequency-inverse document frequency) was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular.\n",
    "\n",
    "However, if the word Bug appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant. For example, if what we’re doing is trying to find out which topics some NPS responses belong to, the word Bug would probably end up being tied to the topic Reliability, since most responses containing that word would be about that topic.\n",
    "\n",
    "#### How is TD-IDF computed?\n",
    "\n",
    "TF-IDF for a word in a document is calculated by multiplying two different metrics:\n",
    "\n",
    "- The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document.\n",
    "- The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n",
    "So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\n",
    "\n",
    "Multiplying these two numbers results in the TF-IDF score of a word in a document. The higher the score, the more relevant that word is in that particular document.\n",
    "\n",
    "To put it in more formal mathematical terms, the TF-IDF score for the word t in the document d from the document set D is calculated as follows:\n",
    "\n",
    "$$tf(t, d) = \\frac{f_d(t)}{\\max_{w \\in d} f_d(w)}$$\n",
    "\n",
    "\n",
    "$$idf(t, D) = \\ln(\\frac{|D|}{|\\{d \\in D: t \\in d\\}|})$$\n",
    "\n",
    "\n",
    "$$tfidf(t, d, D) = tf(t, d) * idf(t, D)$$\n",
    "\n",
    "\n",
    "where $f_d(t)$ is the frequency of term t in document d, and D is the corpus of documents.\n",
    "\n",
    "#### Why does it work?\n",
    "\n",
    "Machine learning with natural language is faced with one major hurdle – its algorithms usually deal with numbers, and natural language is, well, text. So we need to transform that text into numbers, otherwise known as text vectorization. It’s a fundamental step in the process of machine learning for analyzing data, and different vectorization algorithms will drastically affect end results, so you need to choose one that will deliver the results you’re hoping for.\n",
    "\n",
    "Once you’ve transformed words into numbers, in a way that’s machine learning algorithms can understand, the TF-IDF score can be fed to algorithms such as Naive Bayes and Support Vector Machines, greatly improving the results of more basic methods like word counts.\n",
    "\n",
    "#### Basic word frequency count"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 8)\n<class 'scipy.sparse.csr.csr_matrix'>\n[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "source": [
    "#### Word frequency using TDIF-Vectorizer\n",
    "\n",
    "Below is an example of using the TfidfVectorizer to learn vocabulary and inverse document frequencies across 3 small documents and then encode one of those documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\t\"The dog.\",\n",
    "\t\t\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 8)\n[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "source": [
    "A vocabulary of 8 words is learned from the documents and each word is assigned a unique integer index in the output vector.\n",
    "The inverse document frequencies are calculated for each word in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word: “the” at index 7.\n",
    "\n",
    "\n",
    "Finally, the first document is encoded as an 8-element sparse array and we can review the final scorings of each word with different values for “the“, “fox“, and “dog” from the other words in the vocabulary. The scores are normalized to values between 0 and 1 and the encoded document vectors can then be used directly with most machine learning algorithms."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 8)\n<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "vectors = vectorizer.transform([text[0], text[1]])\n",
    "print(vectors.shape)\n",
    "print(type(vectors))"
   ]
  },
  {
   "source": [
    "#### Application to Expedia data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus_before_idf = [' '.join(words) for words in preprocessed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(clean_corpus_before_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of words: 5835\n"
     ]
    }
   ],
   "source": [
    "print('Number of words: %.f' % len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[9.34152942 9.34152942 9.34152942 8.08876645 8.93606431 8.24291713\n 8.24291713 9.34152942 5.09303418 9.34152942]\n5835\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_[:10])\n",
    "print(len(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8388, 5835)\n"
     ]
    }
   ],
   "source": [
    "# each row corresponds to one review and each column to one word \n",
    "corpus_tdif_matrix = vectorizer.transform(clean_corpus_before_idf)\n",
    "print(corpus_tdif_matrix.toarray().shape)"
   ]
  },
  {
   "source": [
    "In practice using TF-IDF-vectors, that have been calculated with the entire corpus (training and test subsets combined), while training the model might introduce some data leakage and hence yield in too optimistic performance measures. This is because the IDF-part of the training set's TF-IDF features will then include information from the test set already.\n",
    "Calculating them completely separately for the training and test set is not a good idea either, because besides testing the quality of your model then you will be also testing the quality of your IDF-estimation. And because the test set is usually small this will be a poor estimation and will worsen your performance measures.\n",
    "\n",
    "Therefore the solution would be (analogously to the common mean imputation of missing values) to perform TF-IDF-normalization on the training set seperately and then use the IDF-vector from the training set to calculate the TF-IDF vectors of the test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}